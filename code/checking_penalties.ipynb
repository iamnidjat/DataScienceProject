{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2e5e04-1597-46fc-8348-227eac81a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5428 rows with NaN values\n",
      "Found 1857 rows with NaN values\n",
      "Found 1848 rows with NaN values\n",
      "                        Feature       VIF\n",
      "21                 n_activities  2.981763\n",
      "20                 total_clicks  2.491377\n",
      "11              region_Scotland  1.866870\n",
      "8          region_London Region  1.813003\n",
      "19               weighted_score  1.778669\n",
      "13          region_South Region  1.750985\n",
      "10  region_North Western Region  1.749844\n",
      "16  region_West Midlands Region  1.654758\n",
      "9           region_North Region  1.586882\n",
      "6   region_East Midlands Region  1.575933\n",
      "14     region_South West Region  1.570907\n",
      "15                 region_Wales  1.536377\n",
      "17      region_Yorkshire Region  1.526794\n",
      "12     region_South East Region  1.524634\n",
      "7                region_Ireland  1.332628\n",
      "1                      imd_band  1.209992\n",
      "0             highest_education  1.118015\n",
      "4               studied_credits  1.073264\n",
      "2                      age_band  1.058426\n",
      "3          num_of_prev_attempts  1.050986\n",
      "5                      gender_M  1.045611\n",
      "22            date_registration  1.036456\n",
      "18                 disability_Y  1.025853\n",
      "Converged in [10] iterations\n",
      "\n",
      "=== Penalty: L1 ===\n",
      "Converged in [12] iterations\n",
      "\n",
      "=== Penalty: L1 ===\n",
      "Converged in [13] iterations\n",
      "\n",
      "=== Penalty: L1 ===\n",
      "Converged in [13] iterations\n",
      "\n",
      "=== Penalty: L1 ===\n",
      "Converged in [13] iterations\n",
      "\n",
      "=== Penalty: L1 ===\n",
      "Converged in [12] iterations\n",
      "\n",
      "=== Penalty: L2 ===\n",
      "Converged in [12] iterations\n",
      "\n",
      "=== Penalty: L2 ===\n",
      "Converged in [12] iterations\n",
      "\n",
      "=== Penalty: L2 ===\n",
      "Converged in [12] iterations\n",
      "\n",
      "=== Penalty: L2 ===\n",
      "Converged in [12] iterations\n",
      "\n",
      "=== Penalty: L2 ===\n",
      "Converged in [12] iterations\n",
      "\n",
      "=== Penalty: ELASTICNET ===\n",
      "Converged in [14] iterations\n",
      "\n",
      "=== Penalty: ELASTICNET ===\n",
      "Converged in [14] iterations\n",
      "\n",
      "=== Penalty: ELASTICNET ===\n",
      "Converged in [14] iterations\n",
      "\n",
      "=== Penalty: ELASTICNET ===\n",
      "Converged in [14] iterations\n",
      "\n",
      "=== Penalty: ELASTICNET ===\n",
      "Penalty none not supported with solver lbfgs: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "Penalty none not supported with solver lbfgs: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "Penalty none not supported with solver lbfgs: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "Penalty none not supported with solver lbfgs: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "Penalty none not supported with solver lbfgs: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "Comparison of penalties:\n",
      "       penalty       C   val_acc  val_roc_auc  val_pr_auc\n",
      "0           l1    0.01  0.854995     0.936980    0.948488\n",
      "1           l1    0.10  0.855148     0.937289    0.949209\n",
      "2           l1    1.00  0.855302     0.937250    0.949233\n",
      "3           l1   10.00  0.855302     0.937246    0.949232\n",
      "4           l1  100.00  0.855302     0.937246    0.949232\n",
      "5           l2    0.01  0.854381     0.937016    0.949122\n",
      "6           l2    0.10  0.855455     0.937232    0.949232\n",
      "7           l2    1.00  0.855302     0.937242    0.949234\n",
      "8           l2   10.00  0.855302     0.937245    0.949236\n",
      "9           l2  100.00  0.855302     0.937245    0.949236\n",
      "10  elasticnet    0.01  0.854688     0.937141    0.948903\n",
      "11  elasticnet    0.10  0.855148     0.937258    0.949213\n",
      "12  elasticnet    1.00  0.855302     0.937246    0.949231\n",
      "13  elasticnet   10.00  0.855302     0.937244    0.949231\n",
      "14  elasticnet  100.00  0.855302     0.937244    0.949231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "def prepare_data(set_dir):\n",
    "    data_dir = Path.home() / \"OneDrive\" / \"Desktop\" / \"Logistic Regression Skewness Fixed\" / \"constant\"\n",
    "    assessments = pd.read_csv(f\"{data_dir}/assessments.csv\")\n",
    "    student_info = pd.read_csv(f\"{set_dir}/student_info.csv\")\n",
    "    student_assessment = pd.read_csv(f\"{set_dir}/student_assessment.csv\")\n",
    "    student_reg = pd.read_csv(f\"{set_dir}/student_reg.csv\")\n",
    "    student_vle = pd.read_csv(f\"{set_dir}/student_vle.csv\")\n",
    "    \n",
    "    student_assessment = pd.merge(\n",
    "        student_assessment, \n",
    "        assessments[['id_assessment', 'weight', 'assessment_type']], \n",
    "        on='id_assessment', \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    student_assessment = student_assessment[student_assessment['weight'] > 0] \n",
    "  # student_assessment = student_assessment[student_assessment['assessment_type'] != 'Exam']\n",
    "\n",
    "    # Aggregate assessments per student\n",
    "    student_agg = student_assessment.groupby(\n",
    "        ['code_module', 'code_presentation', 'id_student']\n",
    "    ).agg(\n",
    "        mean_score=('score', 'mean'),\n",
    "        max_score=('score', 'max'),\n",
    "        min_score=('score', 'min'),\n",
    "        n_assessments=('score', 'count'),\n",
    "        weighted_score=('score', lambda x: (x * student_assessment.loc[x.index, 'weight']).sum() / 100)\n",
    "    ).reset_index()\n",
    "\n",
    "    vle_agg = student_vle.groupby(['code_module', 'code_presentation', 'id_student']).agg(\n",
    "    total_clicks=('sum_click', 'sum'),\n",
    "    n_activities=('id_site', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    merge_keys = ['code_module', 'code_presentation', 'id_student']\n",
    "    df = student_info.merge(student_agg, on=merge_keys, how='left')\n",
    "    df = df.merge(vle_agg, on=merge_keys, how='left')\n",
    "    df = pd.merge(df, student_reg, on=merge_keys, how='left')\n",
    "\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    print(f\"Found {len(nan_rows)} rows with NaN values\")\n",
    "    # print(nan_rows.head())\n",
    "\n",
    "    assessment_cols = ['mean_score', 'max_score', 'min_score', 'weighted_score']\n",
    "    df[assessment_cols] = df[assessment_cols].fillna(-1)  # -1 indicates no assessments\n",
    "    df['n_assessments'] = df['n_assessments'].fillna(0)   # 0 assessments completed\n",
    "    \n",
    "    df['total_clicks'] = df['total_clicks'].fillna(0)\n",
    "    df['n_activities'] = df['n_activities'].fillna(0)\n",
    "\n",
    "    df = df.drop(columns=['date_unregistration', 'mean_score', 'max_score', 'min_score'], errors='ignore')\n",
    "\n",
    "    df = df.drop(columns=['n_assessments']) # because of multicollinearity\n",
    "    # Dropping equity related  features\n",
    "  # df = df.drop(columns=['disability_Y', 'age_band', 'imd_band', 'highest_education', 'gender_M'])\n",
    "    # Dropping regions\n",
    "  # df = df.drop(columns=[reg for reg in df.columns if reg.startswith('region_')])\n",
    "    \n",
    "    y = df['final_result'].apply(lambda x: 1 if x in ['Fail', 'Withdrawn'] else 0)  # binary target\n",
    "\n",
    "    X = df.drop(columns=['code_module', 'code_presentation', 'id_student', 'final_result'])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def compute_vif(scaled_data, original_columns):\n",
    "    # Converts scaled features into DataFrame\n",
    "    X_df = pd.DataFrame(scaled_data, columns=original_columns.columns)\n",
    "    \n",
    "    # Computes VIF for each feature\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X_df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_df.values, i) for i in range(X_df.shape[1])]\n",
    "    \n",
    "    # Shows high-VIF features\n",
    "    print(vif_data.sort_values(\"VIF\", ascending=False))\n",
    "\n",
    "data_dir = Path.home() / \"OneDrive\" / \"Desktop\" / \"Logistic Regression Skewness Fixed\" / \"constant\"\n",
    "\n",
    "X_train, y_train = prepare_data(data_dir / \"train\")\n",
    "X_val, y_val = prepare_data(data_dir / \"val\")\n",
    "X_test, y_test = prepare_data(data_dir / \"test\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting only on training data, then transforming all sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val) \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "compute_vif(X_train_scaled, X_train)\n",
    "\n",
    "penalties = ['l1', 'l2', 'elasticnet', 'none']\n",
    "results = []\n",
    "\n",
    "for penalty in penalties:\n",
    "    for C in [0.01, 0.1, 1, 10, 100]:\n",
    "        # solver must match penalty type\n",
    "        if penalty == 'l1':\n",
    "            solver = 'liblinear'\n",
    "        elif penalty == 'elasticnet':\n",
    "            solver = 'saga'\n",
    "        elif penalty in ['l2', 'none']:\n",
    "            solver = 'lbfgs' \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            # Training\n",
    "            model = LogisticRegression(\n",
    "                penalty=penalty,\n",
    "                C=C,                           \n",
    "                l1_ratio=0.5 if penalty == 'elasticnet' else None,  # only for elasticnet\n",
    "                class_weight='balanced',         # Handles class imbalance\n",
    "                solver=solver,                 \n",
    "                max_iter=100,                    # Allows enough iterations to converge\n",
    "                random_state=42                  # For reproducibility\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            print(f'Converged in {model.n_iter_} iterations')\n",
    "        \n",
    "            print(f\"\\n=== Penalty: {penalty.upper()} ===\")\n",
    "            y_val_probs = model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "            results.append({\n",
    "                \"penalty\": penalty,\n",
    "                \"C\": C,\n",
    "                \"val_acc\": accuracy_score(y_val, model.predict(X_val_scaled)),\n",
    "                \"val_roc_auc\": roc_auc_score(y_val, y_val_probs),\n",
    "                \"val_pr_auc\": average_precision_score(y_val, y_val_probs)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Penalty {penalty} not supported with solver {solver}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison of penalties:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1bd614-1400-497c-b7f7-3f31e326736c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c3092-9df4-4f26-8247-fd7bb8fa7e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968abec-99f8-4548-8302-a5a3889840e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8da9c-411f-463b-81b4-46b104ac0123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f90ae-d80e-4bd8-a219-84a31392bc68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7b410-3f59-4928-aae9-afb1cebf3342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb030dea-cda0-4e22-aee4-368ffdc58981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4daa38-b894-43a5-835b-05551f16f503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (text_analytics)",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
